{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "arms = np.random.rand(n)\n",
    "eps = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reward(prob):\n",
    "    reward = 0\n",
    "    for i in range(10):\n",
    "        if random.random() < prob:\n",
    "            reward += 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# action-value\n",
    "av = np.array([np.random.randint(0, (n + 1)), 0]).reshape(1, 2)\n",
    "\n",
    "def best_arm(a):\n",
    "    best_arm = 0\n",
    "    best_mean = 0\n",
    "    for u in a:\n",
    "        # mean reward for each action\n",
    "        avg = np.mean(a[np.where(a[:, 0] == u[0])][:, 1])\n",
    "        if best_mean < avg:\n",
    "            best_mean = avg\n",
    "            best_arm = u[0]\n",
    "    return best_arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEPCAYAAABLIROyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10da68e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Plays')\n",
    "plt.ylabel('Avg Reward')\n",
    "for i in range(500):\n",
    "    if random.random() > eps: # greedy arm selection\n",
    "        choice = best_arm(av)\n",
    "    else: # random arm selection\n",
    "        choice = np.where(arms == np.random.choice(arms))[0][0]\n",
    "    this_av = np.array([[choice, reward(arms[choice])]])\n",
    "    av = np.concatenate((av, this_av), axis=0)\n",
    "    # percentage the correct arm is chosen\n",
    "    perc_correct = 100 * (len(av[np.where(av[:, 0] == np.argmax(arms))]) * 1. / len(av))\n",
    "    # mean reward\n",
    "    running_mean = np.mean(av[:, 1])\n",
    "    plt.scatter(i, running_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# experiment with different numbers of arms and different values for ϵ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The problem we've considered here is a stationary problem \n",
    "# because the underlying reward probability distributions for each arm do not change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We certainly could consider a variant of this problem where this is not true, \n",
    "# a non-stationary problem. In this case, a simple modification would be \n",
    "# to weight more recent action-value pairs greater than distant ones, \n",
    "# thus if things change over time, we will be able to track them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEPCAYAAAC3NDh4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQkREMlKSEBGRjJQkREQkIyUJERHJ6P8D6UnZI4nNqucAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ded4750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10\n",
    "arms = np.random.rand(n)\n",
    "eps = 0.1\n",
    "\n",
    "av = np.ones(n) # action-value\n",
    "counts = np.zeros(n) # how many times we've taken a particular action\n",
    "\n",
    "def reward(prob):\n",
    "    total = 0\n",
    "    for i in range(10):\n",
    "        if random.random() < prob:\n",
    "            total += 1\n",
    "    return total\n",
    "\n",
    "# simpler best_arm function\n",
    "def best_arm(a):\n",
    "    return np.argmax(a)\n",
    "\n",
    "plt.xlabel('Plays')\n",
    "plt.ylabel('Mean Reward')\n",
    "for i in range(500):\n",
    "    if random.random() > eps:\n",
    "        choice = best_arm(av)\n",
    "    else:\n",
    "        choice = np.where(arms == np.random.choice(arms))[0][0]\n",
    "    counts[choice] += 1\n",
    "    k = counts[choice]\n",
    "    rwd = reward(arms[choice])\n",
    "    old_avg = av[choice]\n",
    "    new_avg = old_avg + (1. / k) * (rwd - old_avg)\n",
    "    av[choice] = new_avg\n",
    "    # weighted average\n",
    "    running_mean = np.average(av, weights=np.array([counts[j] * 1. / np.sum(counts) for j in range(len(counts))]))\n",
    "    plt.scatter(i, running_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# τ is a parameter called temperature the scales the probability distribution of actions. \n",
    "# A high temperature will tend the probabilities to be very simmilar, whereas a low temperature \n",
    "# will exaggerate differences in probabilities between actions. Selecting this parameter requires \n",
    "# an educated guess and some trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEPCAYAAABLIROyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d88c750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10\n",
    "arms = np.random.rand(n)\n",
    "\n",
    "av = np.ones(n) # action-value\n",
    "counts = np.zeros(n) # how many times we've taken a particular action\n",
    "av_softmax = np.zeros(n)\n",
    "av_softmax[:] = 0.1 # initial probability\n",
    "\n",
    "def reward(prob):\n",
    "    total = 0\n",
    "    for i in range(10):\n",
    "        if random.random() < prob:\n",
    "            total += 1\n",
    "    return total\n",
    "\n",
    "tau = 1.12\n",
    "def softmax(av):\n",
    "    normalization_factor = np.sum(np.exp(av[:] / tau))\n",
    "    probs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        probs[i] = np.exp(av[i] / tau) / normalization_factor\n",
    "    return probs\n",
    "\n",
    "plt.xlabel('Plays')\n",
    "plt.ylabel('Mean Reward')\n",
    "for i in range(500):\n",
    "    choice = np.where(arms == np.random.choice(arms, p=av_softmax))[0][0]\n",
    "    counts[choice] += 1\n",
    "    k = counts[choice]\n",
    "    rwd = reward(arms[choice])\n",
    "    old_avg = av[choice]\n",
    "    new_avg = old_avg + (1. / k) * (rwd - old_avg)\n",
    "    av[choice] = new_avg\n",
    "    av_softmax = softmax(av)\n",
    "    \n",
    "    running_mean = np.average(av, weights=np.array([counts[j] * 1. / np.sum(counts) for j in range(len(counts))]))\n",
    "    plt.scatter(i, running_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax action selection seems to do at least as well as epsilon-greedy, \n",
    "# perhaps even better; it looks like it converges on an optimal policy faster. \n",
    "# The downside to softmax is having to manually select the τ parameter. \n",
    "# Softmax here was pretty sensitive to τ and it took awhile of playing with it\n",
    "# to find a good value for it. Obviously with epsilon-greedy we had the parameter\n",
    "# epsilon to set, but choosing that parameter was much more intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The state space for 21 is much much larger than the single state in n-armed bandit.\n",
    "# In RL, a state is all information available to the agent (the decision maker) at a particular time t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So what are all the possible combinations of information available to the agent (the player) in blackjack? \n",
    "# Well, the player starts with two cards, so there is the combination of all 2 playing cards. \n",
    "# Additionally, the player knows one of the two cards that the dealer has. \n",
    "# Thus, there are a lot of possible states (around 200). \n",
    "# As with any RL problem, our ultimate goal is to find the best policy to maximize our rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our main computational effort, therefore, is in iteratively improving our estimates for the values \n",
    "# of states or state-action pairs.\n",
    "# For example, given the cards total to 20, what is the value of hitting vs staying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problems like the n-armed bandit problem and blackjack have a small enough state or state-action space \n",
    "# that we can record and average rewards in a lookup table, giving us the exact average rewards for \n",
    "# each state-action pair. Most interesting problems, however, have a state space that is continuous or \n",
    "# otherwise too large to use a lookup table. That's when we must use function approximation \n",
    "# (e.g. neural networks) methods to serve as our  QQ  function in determining the value of states or state-actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is why DeepMind's implementation actually feeds in the last 4 frames of gameplay, \n",
    "# effectively changing a non-Markov decision process into an MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Qk(s,a)Qk(s,a) is the function that accepts an action and state and returns the value of \n",
    "# taking that action in that state at time step kk. This is fundamental to RL. \n",
    "# We need to know the relative values of every state or state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# π is a policy, a stochastic strategy or rule to choose action a given a state s. \n",
    "# Think of it as a function, π(s), that accepts state, s and returns the action to be taken. \n",
    "# There is a distinction between the π(s) function and a specific policy π. Our implementation \n",
    "# of π(s) as a function is often to just choose the action a in state s that has the highest \n",
    "# average return based on historical results, argmaxQ(s,a). As we gather more data and \n",
    "# these average returns become more accurate, the actual policy π may change. We may \n",
    "# start out with a policy of \"hit until total is 16 or more then stay\" but this policy \n",
    "# may change as we gather more data. Our implemented π(s) function, however, \n",
    "# is programmed by us and does not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gt, cumulative return starting from a given state until the end of an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Episode: the full sequence of steps leading to a terminal state and receiving a return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vπ, a function that determines the value of a state given a policy π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Monte Carlo\n",
    "# We'll use random sampling of states and state-action pairs\n",
    "# and observe rewards and then iteratively revise our policy,\n",
    "# which will hopefully **converge** on the optimal policy \n",
    "# as we explore every possible state-action couple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code is functional and stateless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "def random_card():\n",
    "    card = random.randint(1, 13)\n",
    "    if card > 10:\n",
    "        card = 10\n",
    "    return card\n",
    "\n",
    "def useable_ace(hand):\n",
    "    val, ace = hand\n",
    "    return ace and val + 10 <= 21\n",
    "\n",
    "def total_value(hand):\n",
    "    val, ace = hand\n",
    "    if useable_ace(hand):\n",
    "        return val + 10\n",
    "    else:\n",
    "        return val\n",
    "    \n",
    "def add_card(hand, card):\n",
    "    val, ace = hand\n",
    "    if card == 1:\n",
    "        ace = True\n",
    "    return (val + card, ace)\n",
    "\n",
    "def eval_dealer(dealer_hand):\n",
    "    while total_value(dealer_hand) < 17:\n",
    "        dealer_hand = add_card(dealer_hand, random_card())\n",
    "    return dealer_hand\n",
    "\n",
    "def play(state, dec):\n",
    "    player_hand = state[0]\n",
    "    dealer_hand = state[1]\n",
    "    if dec == 0: # 1 hit, 0 stay\n",
    "        dealer_hand = eval_dealer(dealer_hand)\n",
    "        \n",
    "        player_tot = total_value(player_hand)\n",
    "        dealer_tot = total_value(dealer_hand)\n",
    "        status = 1 # 1 game is on, 2 play won, 3 draw, 4 dealer won\n",
    "        if dealer_tot > 21 or dealer_tot < player_tot:\n",
    "            status = 2\n",
    "        elif dealer_tot == player_tot:\n",
    "            status = 3\n",
    "        elif dealer_tot > player_tot:\n",
    "            status = 4\n",
    "    elif dec == 1:\n",
    "        player_hand = add_card(player_hand, random_card())\n",
    "        dealer_hand = eval_dealer(dealer_hand)\n",
    "        \n",
    "        player_tot = total_value(player_hand)\n",
    "        dealer_tot = total_value(dealer_hand)\n",
    "        status = 1\n",
    "        if player_tot == 21:\n",
    "            if dealer_tot == 21:\n",
    "                status = 3\n",
    "            else:\n",
    "                status = 2\n",
    "        elif player_tot > 21:\n",
    "            status = 4\n",
    "        elif player_tot < 21:\n",
    "            pass # game continues\n",
    "    state = (player_hand, dealer_hand, status)\n",
    "    return state\n",
    "\n",
    "def init_game():\n",
    "    status = 1\n",
    "    player_hand = add_card((0, False), random_card())\n",
    "    player_hand = add_card(player_hand, random_card())\n",
    "    dealer_hand = add_card((0, False), random_card())\n",
    "    \n",
    "    if total_value(player_hand) == 21:\n",
    "        if total_value(dealer_hand) != 21:\n",
    "            status = 2\n",
    "        else:\n",
    "            status = 3\n",
    "            \n",
    "    state = (player_hand, dealer_hand, status)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((19, False), (10, False), 1)\n"
     ]
    }
   ],
   "source": [
    "state = init_game()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((21, False), (20, False), 2)\n"
     ]
    }
   ],
   "source": [
    "state = play(state, 1)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will compress the states a bit by ignoring the useable ace boolean \n",
    "# for the dealer's hand because the dealer only shows a single card and \n",
    "# if it's an ace the player has no idea if it's useable or not, so it \n",
    "# offers no additional information to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Monte Carlo Reinforcement Learning\n",
    "# use an epsilon-greedy policy function to ensure \n",
    "# we have a good balance of exploration versus exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In essence, with Monte Carlo we are playing randomly initialized games, \n",
    "# sampling the state-action pair space and recording returns. In doing so, \n",
    "# we can iteratively update our policy π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def init_state_space():\n",
    "    states = []\n",
    "    for card in range(1, 11):\n",
    "        for val in range(11, 22):\n",
    "            states.append((val, False, card))\n",
    "            states.append((val, True, card))\n",
    "    return states\n",
    "\n",
    "def init_state_actions(states):\n",
    "    av = {}\n",
    "    for state in states:\n",
    "        av[(state, 0)] = 0.0\n",
    "        av[(state, 1)] = 0.0\n",
    "    return av\n",
    "\n",
    "def init_SA_count(state_actions):\n",
    "    counts = {}\n",
    "    for sa in state_actions:\n",
    "        counts[sa] = 0\n",
    "    return counts\n",
    "\n",
    "# reward = 1 for winning, 0 for draw, -1 for losing\n",
    "def calc_reward(outcome):\n",
    "    return 3 - outcome\n",
    "\n",
    "def update_Q_table(av_table, av_count, returns):\n",
    "    for key in returns:\n",
    "        av_table[key] = av_table[key] + (1. / av_count[key]) * (returns[key] - av_table[key])\n",
    "    return av_table\n",
    "\n",
    "# avg rewards - Q-value for each action given a state\n",
    "def qsv(state, av_table):\n",
    "    if (state, 0) not in av_table:\n",
    "        av_table[(state, 0)] = 0\n",
    "    if (state, 1) not in av_table:\n",
    "        av_table[(state, 1)] = 0\n",
    "    stay = av_table[(state, 0)]\n",
    "    hit = av_table[(state, 1)]\n",
    "    return np.array([stay, hit])\n",
    "\n",
    "# compress the state\n",
    "def get_RL_state(state):\n",
    "    player_hand, dealer_hand, status = state\n",
    "    player_val, player_ace = player_hand\n",
    "    return (player_val, player_ace, dealer_hand[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000000\n",
    "epsilon = 0.1\n",
    "\n",
    "state_space = init_state_space()\n",
    "av_table = init_state_actions(state_space)\n",
    "av_count = init_SA_count(av_table)\n",
    "\n",
    "for i in range(epochs):\n",
    "    state = init_game()\n",
    "    player_hand, dealer_hand, status = state\n",
    "    \n",
    "    while player_hand[0] < 11:\n",
    "        player_hand = add_card(player_hand, random_card())\n",
    "        state = (player_hand, dealer_hand, status)\n",
    "    rl_state = get_RL_state(state)\n",
    "    \n",
    "    returns = {}\n",
    "    while state[2] == 1:\n",
    "        act_probs = qsv(rl_state, av_table)\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randint(0, 1)\n",
    "        else:\n",
    "            action = np.argmax(act_probs)\n",
    "        sa = (rl_state, action)\n",
    "        returns[sa] = 0\n",
    "        if sa not in av_count:\n",
    "            av_count[sa] = 0\n",
    "        av_count[sa] += 1\n",
    "        state = play(state, action)\n",
    "        rl_state = get_RL_state(state)\n",
    "    \n",
    "    for key in returns.keys():\n",
    "        returns[key] = calc_reward(state[2])\n",
    "        \n",
    "    av_table = update_Q_table(av_table, av_count, returns)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize=(8, 6))\n",
    "#ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "#ax.set_xlabel('Dealer card')\n",
    "#ax.set_ylabel('Player sum')\n",
    "#ax.set_zlabel('State-Value')\n",
    "\n",
    "x,y,z,a = [],[],[],[]\n",
    "for key in state_space:\n",
    "    if (not key[1] and key[0] > 11 and key[2] < 21):\n",
    "        y.append(key[0])\n",
    "        x.append(key[2])\n",
    "        state_value = max([av_table[(key, 0)], av_table[(key, 1)]])\n",
    "        z.append(state_value)\n",
    "        if av_table[(key, 0)] >= av_table[(key, 1)]:\n",
    "            a.append(0)\n",
    "        else:\n",
    "            a.append(1)\n",
    "#ax.azim = 230\n",
    "#ax.plot_trisurf(x,y,z, linewidth=.02, cmap=cm.jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we've covered Monte Carlo reinforcement learning methods that depending on stochastically\n",
    "# sampling the environment and iteratively improving a policy π after each episode. One \n",
    "# disadvantage of Monte Carlo methods is that we must wait until the end of an *episode* \n",
    "# to update our policy. For some types of problems (like blackjack), this is okay, but \n",
    "# in a lot of cases, it makes more sense to be able to learn at each time step (immediately \n",
    "# after each action is taken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Dealer card</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Player sum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Dealer card  1   2   3   4   5   6   7   8   9   10\n",
       "Player sum                                         \n",
       "12            1   1   1   1   1   1   1   1   1   1\n",
       "13            1   1   1   1   0   0   1   1   1   1\n",
       "14            1   0   0   0   0   0   1   1   1   1\n",
       "15            1   0   0   0   0   0   1   1   1   1\n",
       "16            1   0   0   0   0   0   1   1   1   1\n",
       "17            0   0   0   0   0   0   0   0   0   0\n",
       "18            0   0   0   0   0   0   0   0   0   0\n",
       "19            0   0   0   0   0   0   0   0   0   0\n",
       "20            0   0   0   0   0   0   0   0   0   0\n",
       "21            0   0   0   0   0   0   0   0   0   0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "strategy = pd.DataFrame(zip(x, y, z, a), columns=['Dealer card', 'Player sum', 'State-Value', 'Policy'])\n",
    "strategy.pivot(index='Player sum', columns='Dealer card', values='Policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The most important thing to learn from all of this is that in \n",
    "# essentially any RL method, our goal is to find an optimal Q function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In the next part, I will abandon tabular learning methods and cover \n",
    "# Q-learning (a type of temporal difference (TD) algorithm) using a neural\n",
    "# network as our Q function (what we've all been waiting for)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural nets provide a functional approximator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our Q function actually looks like this: Q(s,a,θ) where θ is \n",
    "# a vector of parameters. And instead of iteratively updating values \n",
    "# in a table, we will iteratively update the θ parameters of \n",
    "# our neural network so that it learns to provide us with better \n",
    "# estimates of state-action values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target: r_t+1 + γ ∗ maxQ(s′, a′) for non-terminal states\n",
    "        r_t+1 for terminal states (last state in an episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# γ is a parameter 0-→1 that is called the discount factor. \n",
    "# Basically it determines how much each future reward is taken \n",
    "# into consideration for updating our Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If γ is close to 0, we heavily discount future rewards and \n",
    "# thus mostly care about immediate rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# s′ refers to the new state after having taken action a \n",
    "# and a′ refers to the next actions possible in this new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So maxQ(s′, a′) means we calculate all the Q-values for each \n",
    "# state-action pair in the new state, and take the maximium value \n",
    "# to use in our new value update. \n",
    "# (Note I may use s′ and a′ interchangeably with s_t+1 and a_t+1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In on-policy methods we iteratively learn about state values \n",
    "# at the same time that we improve our policy. In other words, \n",
    "# the updates to our state values depend on the policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In contrast, off-policy methods do not depend on the policy \n",
    "# to update the value function. Q-learning is an **off-policy** method. \n",
    "# It's advantageous because with off-policy methods, we can follow \n",
    "# one policy while learning about __another__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For example, with Q-learning, we could always take completely random\n",
    "# actions and yet we would still learn about another policy function \n",
    "# of taking the best actions in every state. If there's ever a π \n",
    "# referenced in the value update part of the algorithm then it's \n",
    "# an on-policy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rand_pair(s, e):\n",
    "    return np.random.randint(s, e), np.random.randint(s, e)\n",
    "\n",
    "# finds an array in the \"depth\" dimension of the grid\n",
    "def find_loc(state, obj):\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if all(state[i, j] == obj):\n",
    "                return i, j\n",
    "            \n",
    "# initialize stationary grid, all items are placed deterministically\n",
    "def init_grid():\n",
    "    state = np.zeros((4, 4, 4))\n",
    "    # place player\n",
    "    state[0, 1] = np.array([0, 0, 0, 1])\n",
    "    # place wall\n",
    "    state[2, 2] = np.array([0, 0, 1, 0])\n",
    "    # place pit\n",
    "    state[1, 1] = np.array([0, 1, 0, 0])\n",
    "    # place goal\n",
    "    state[3, 3] = np.array([1, 0, 0, 0])\n",
    "    \n",
    "    return state\n",
    "\n",
    "# initialize player in random location, but keep wall, goal and pit stationary\n",
    "def init_grid_player():\n",
    "    state = np.zeros((4, 4, 4))\n",
    "    # place player\n",
    "    state[rand_pair(0, 4)] = np.array([0, 0, 0, 1])\n",
    "    # place wall\n",
    "    state[2, 2] = np.array([0, 0, 1, 0])\n",
    "    # place pit\n",
    "    state[1, 1] = np.array([0, 1, 0, 0])\n",
    "    # place goal\n",
    "    state[1, 2] = np.array([1, 0, 0, 0])\n",
    "    \n",
    "    # find grid position of player (agent)\n",
    "    a = find_loc(state, np.array([0, 0, 0, 1])) \n",
    "    # find wall\n",
    "    w = find_loc(state, np.array([0, 0, 1, 0]))\n",
    "    # find goal\n",
    "    g = find_loc(state, np.array([1, 0, 0, 0]))\n",
    "    # find pit\n",
    "    p = find_loc(state, np.array([0, 1, 0, 0]))\n",
    "    \n",
    "    if not all([a, w, g, p]):\n",
    "        print('Invalid grid. Rebuilding...')\n",
    "        return init_grid_player()\n",
    "    \n",
    "    return state\n",
    "\n",
    "# initialize grid so that goal, pit, wall, player are all randomly placed\n",
    "def init_grid_rand():\n",
    "    state = np.zeros((4, 4, 4))\n",
    "    # place player\n",
    "    state[rand_pair(0, 4)] = np.array([0, 0, 0, 1])\n",
    "    # place wall\n",
    "    state[rand_pair(0, 4)] = np.array([0, 0, 1, 0])\n",
    "    # place pit\n",
    "    state[rand_pair(0, 4)] = np.array([0, 1, 0, 0])\n",
    "    # place goal\n",
    "    state[rand_pair(0, 4)] = np.array([1, 0, 0, 0])\n",
    "    \n",
    "    a = find_loc(state, np.array([0, 0, 0, 1]))\n",
    "    w = find_loc(state, np.array([0, 0, 1, 0]))\n",
    "    g = find_loc(state, np.array([1, 0, 0, 0]))\n",
    "    p = find_loc(state, np.array([0, 1, 0, 0]))\n",
    "    \n",
    "    # if any of the \"objects\" are superimposed, just call the function again to re-place\n",
    "    if not all([a, w, g, p]):\n",
    "        print('Invalid grid. Rebuilding...')\n",
    "        return init_grid_rand()\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_move(state, action):\n",
    "    player_loc = find_loc(state, np.array([0, 0, 0, 1]))\n",
    "    wall_loc = find_loc(state, np.array([0, 0, 1, 0]))\n",
    "    goal_loc = find_loc(state, np.array([1, 0, 0, 0]))\n",
    "    pit_loc = find_loc(state, np.array([0, 1, 0, 0]))\n",
    "    state = np.zeros((4, 4, 4))\n",
    "    \n",
    "    # up --> row - 1\n",
    "    if action == 0:\n",
    "        new_loc = (player_loc[0] - 1, player_loc[1])\n",
    "    # down --> row + 1\n",
    "    elif action == 1:\n",
    "        new_loc = (player_loc[0] + 1, player_loc[1])\n",
    "    # left --> column - 1\n",
    "    elif action == 2:\n",
    "        new_loc = (player_loc[0], player_loc[1] - 1)\n",
    "    # right --> column + 1\n",
    "    elif action == 3:\n",
    "        new_loc = (player_loc[0], player_loc[1] + 1)\n",
    "        \n",
    "    if new_loc != wall_loc:\n",
    "        if (np.array(new_loc) <= (3, 3)).all() and (np.array(new_loc) >= (0, 0)).all():\n",
    "            state[new_loc][3] = 1\n",
    "                \n",
    "    new_player_loc = find_loc(state, np.array([0, 0, 0, 1]))\n",
    "    if not new_player_loc:\n",
    "        state[player_loc] = np.array([0, 0, 0, 1])\n",
    "    \n",
    "    state[pit_loc][1] = 1\n",
    "    state[wall_loc][2] = 1\n",
    "    state[goal_loc][0] = 1\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_loc(state, level):\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if state[i, j][level] == 1:\n",
    "                return i, j\n",
    "            \n",
    "def get_reward(state):\n",
    "    player_loc = get_loc(state, 3)\n",
    "    pit_loc = get_loc(state, 1)\n",
    "    goal_loc = get_loc(state, 0)\n",
    "    \n",
    "    if player_loc == pit_loc:\n",
    "        return -10\n",
    "    elif player_loc == goal_loc:\n",
    "        return 10\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def disp_grid(state):\n",
    "    grid = np.zeros((4, 4), dtype='<U2')\n",
    "    player_loc = find_loc(state, np.array([0, 0, 0, 1]))\n",
    "    wall_loc = find_loc(state, np.array([0, 0, 1, 0]))\n",
    "    goal_loc = find_loc(state, np.array([1, 0, 0, 0]))\n",
    "    pit_loc = find_loc(state, np.array([0, 1, 0, 0]))\n",
    "        \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            grid[i, j] = ' '\n",
    "            \n",
    "    if player_loc:\n",
    "        grid[player_loc] = 'P'\n",
    "    if wall_loc:\n",
    "        grid[wall_loc] = 'W'\n",
    "    if goal_loc:\n",
    "        grid[goal_loc] = '+'\n",
    "    if pit_loc:\n",
    "        grid[pit_loc] = '-'\n",
    "        \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid grid. Rebuilding...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[u' ', u'P', u' ', u' '],\n",
       "       [u'-', u' ', u' ', u' '],\n",
       "       [u' ', u' ', u' ', u'+'],\n",
       "       [u' ', u'W', u' ', u' ']], \n",
       "      dtype='<U2')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = init_grid_rand()\n",
    "disp_grid(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[u' ', u' ', u' ', u' '],\n",
       "       [u'-', u' ', u' ', u' '],\n",
       "       [u' ', u' ', u' ', u' '],\n",
       "       [u' ', u'W', u' ', u' ']], \n",
       "      dtype='<U2')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = make_move(state, 3)\n",
    "state = make_move(state, 3)\n",
    "state = make_move(state, 1)\n",
    "state = make_move(state, 1)\n",
    "print('Reward: %s' % get_reward(state))\n",
    "disp_grid(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# An input layer of 64 units (because our state has a total of \n",
    "# 64 elements, remember its a 4x4x4 numpy array), 2 hidden layers \n",
    "# of 164 and 150 units, and an output layer of 4, one for each of \n",
    "# our possible actions (up, down, left, right) [in that order].\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(164, init='lecun_uniform', input_shape=(64,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(150, init='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(4, init='lecun_uniform'))\n",
    "model.add(Activation('linear')) # real-valued outputs\n",
    "\n",
    "rms = RMSprop()\n",
    "model.compile(loss='mse', optimizer=rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04001465, -0.13536021, -0.18622869,  0.18035182]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = init_grid_rand()\n",
    "model.predict(state.reshape(1, 64), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game #: 839\n",
      "Epoch 1/1\n",
      "\r",
      "1/1 [==============================] - 0s - loss: 0.3526"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "epochs = 1000\n",
    "gamma = 0.9 # coz it may take several moves to reach goal\n",
    "epsilon = 1\n",
    "\n",
    "for i in range(epochs):\n",
    "    state = init_grid()\n",
    "    status = 1 # game in progress\n",
    "    \n",
    "    while (status) == 1:\n",
    "        # run Q function on S to get Q values for all possible actions\n",
    "        qval = model.predict(state.reshape(1, 64), batch_size=1)\n",
    "        if random.random() < epsilon: # explore\n",
    "            action = np.random.randint(0, 4)\n",
    "        else: # exploit\n",
    "            action = np.argmax(qval)\n",
    "        # take action, observe new state S'\n",
    "        new_state = make_move(state, action)\n",
    "        # observe reward\n",
    "        reward = get_reward(new_state)\n",
    "        # get max_Q(S', a)\n",
    "        new_Q = model.predict(new_state.reshape(1, 64), batch_size=1)\n",
    "        max_Q = np.max(new_Q)\n",
    "        y = np.zeros((1, 4))\n",
    "        y[:] = qval[:]\n",
    "        if reward == -1: # non-terminal\n",
    "            update = reward + gamma * max_Q\n",
    "        else: # terminal\n",
    "            update = reward\n",
    "        y[0][action] = update # target output\n",
    "        print('Game #: %s' % i)\n",
    "        model.fit(state.reshape(1, 64), y, batch_size=1, nb_epoch=1, verbose=1)\n",
    "        state = new_state\n",
    "        if reward != -1:\n",
    "            status = 0\n",
    "        clear_output(wait=True)\n",
    "    if epsilon > 0.1:\n",
    "        epsilon -= (1. / epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_algo(init=0):\n",
    "    i = 0\n",
    "    if init == 0:\n",
    "        state = init_grid()\n",
    "    elif init == 1:\n",
    "        state = init_grid_player()\n",
    "    elif init == 2:\n",
    "        state = init_grid_rand()\n",
    "        \n",
    "    print('Initial State:')\n",
    "    print(disp_grid(state))\n",
    "    status = 1\n",
    "    \n",
    "    while status == 1:\n",
    "        qval = model.predict(state.reshape(1, 64), batch_size=1)\n",
    "        action = np.argmax(qval)\n",
    "        print('Move #: %s; Taking action: %s' % (i, action))\n",
    "        \n",
    "        state = make_move(state, action)\n",
    "        print(disp_grid(state))\n",
    "        \n",
    "        reward = get_reward(state)\n",
    "        if reward != -1:\n",
    "            status = 0\n",
    "            print('Reward: %s' % reward)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        if i > 10:\n",
    "            print('Game lost; too many moves.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "[[u' ' u'P' u' ' u' ']\n",
      " [u' ' u'-' u' ' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u'+']]\n",
      "Move #: 0; Taking action: 3\n",
      "[[u' ' u' ' u'P' u' ']\n",
      " [u' ' u'-' u' ' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u'+']]\n",
      "Move #: 1; Taking action: 1\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u'P' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u'+']]\n",
      "Move #: 2; Taking action: 3\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u' ' u'P']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u'+']]\n",
      "Move #: 3; Taking action: 1\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u' ' u' ']\n",
      " [u' ' u' ' u'W' u'P']\n",
      " [u' ' u' ' u' ' u'+']]\n",
      "Move #: 4; Taking action: 1\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u' ' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u' ']]\n",
      "Reward: 10\n"
     ]
    }
   ],
   "source": [
    "test_algo(init=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# soooooooo magical..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# catastrophic forgetting:\n",
    "#     a push-pull between very similar state-actions \n",
    "#    (but with divergent targets) that results in this \n",
    "#    inability to properly learn anything.\n",
    "# experience replay:\n",
    "#     basically gives us minibatch updating in an \n",
    "#     online learning scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Thus, in addition to learning the action-value for the action \n",
    "# we just took, we're also going to use a random sample of our \n",
    "# past experiences to train on to prevent catastrophic forgetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game #: 2999\n",
      "Epoch 1/1\n",
      "40/40 [==============================] - 0s - loss: 0.2800\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=rms) # reset weights\n",
    "epochs = 3000\n",
    "gamma = 0.975\n",
    "epsilon = 1\n",
    "batch_size = 40\n",
    "buffer_size = 80\n",
    "replay = [] # (S, A, R, S')\n",
    "h = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    state = init_grid_player()\n",
    "    status = 1\n",
    "    \n",
    "    while status == 1:\n",
    "        qval = model.predict(state.reshape(1, 64), batch_size=1)\n",
    "        if random.random() < epsilon:\n",
    "            action = np.random.randint(0, 4)\n",
    "        else:\n",
    "            action = np.argmax(qval)\n",
    "        new_state = make_move(state, action)\n",
    "        reward = get_reward(new_state)\n",
    "        \n",
    "        # experience replay\n",
    "        if len(replay) < buffer_size:\n",
    "            replay.append((state, action, reward, new_state))\n",
    "        else:\n",
    "            if h < buffer_size - 1:\n",
    "                h += 1\n",
    "            else:\n",
    "                h = 0 # circular buffer\n",
    "            replay[h] = (state, action, reward, new_state)\n",
    "            \n",
    "            # randomly sample our experience replay memory\n",
    "            minibatch = random.sample(replay, batch_size)\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            \n",
    "            for memory in minibatch:\n",
    "                old_state, action, reward, new_state = memory\n",
    "                old_qval = model.predict(old_state.reshape(1, 64), batch_size=1)\n",
    "                new_Q = model.predict(new_state.reshape(1, 64), batch_size=1)\n",
    "                max_Q = np.max(new_Q)\n",
    "                y = np.zeros((1, 4))\n",
    "                \n",
    "                if reward == -1:\n",
    "                    update = reward + gamma * max_Q\n",
    "                else:\n",
    "                    update = reward\n",
    "                y[0][action] = update\n",
    "                X_train.append(old_state.reshape(64))\n",
    "                y_train.append(y.reshape(4))\n",
    "                \n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "            print('Game #: %s' % i)\n",
    "            \n",
    "            model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1, verbose=1)\n",
    "            state = new_state\n",
    "        if reward != -1:\n",
    "            status = 0\n",
    "        clear_output(wait=True)\n",
    "    if epsilon > 0.1:\n",
    "        epsilon -= (1. / epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid grid. Rebuilding...\n",
      "Initial State:\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u'+' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u'P' u' ' u' ']]\n",
      "Move #: 0; Taking action: 3\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u'+' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u'P' u' ']]\n",
      "Move #: 1; Taking action: 3\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u'+' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u'P']]\n",
      "Move #: 2; Taking action: 0\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u'+' u' ']\n",
      " [u' ' u' ' u'W' u'P']\n",
      " [u' ' u' ' u' ' u' ']]\n",
      "Move #: 3; Taking action: 0\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u'+' u'P']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u' ']]\n",
      "Move #: 4; Taking action: 2\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u' ' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u' ']]\n",
      "Reward: 10\n"
     ]
    }
   ],
   "source": [
    "test_algo(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# magical !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "[[u' ' u' ' u' ' u'P']\n",
      " [u' ' u'-' u'+' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u' ']]\n",
      "Move #: 0; Taking action: 2\n",
      "[[u' ' u' ' u'P' u' ']\n",
      " [u' ' u'-' u'+' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u' ']]\n",
      "Move #: 1; Taking action: 1\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u' ' u' ']\n",
      " [u' ' u' ' u'W' u' ']\n",
      " [u' ' u' ' u' ' u' ']]\n",
      "Reward: 10\n"
     ]
    }
   ],
   "source": [
    "test_algo(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need GPU to train the hardest variant with more epochs (>50K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game #: 49999\n",
      "Epoch 1/1\n",
      "40/40 [==============================] - 0s - loss: 2.0177\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "model.compile(loss='mse', optimizer=rms) # reset weights\n",
    "epochs = 50000\n",
    "gamma = 0.975\n",
    "epsilon = 1\n",
    "batch_size = 40\n",
    "buffer_size = 80\n",
    "replay = [] # (S, A, R, S')\n",
    "h = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    state = init_grid_rand()\n",
    "    status = 1\n",
    "    \n",
    "    while status == 1:\n",
    "        qval = model.predict(state.reshape(1, 64), batch_size=1)\n",
    "        if random.random() < epsilon:\n",
    "            action = np.random.randint(0, 4)\n",
    "        else:\n",
    "            action = np.argmax(qval)\n",
    "        new_state = make_move(state, action)\n",
    "        reward = get_reward(new_state)\n",
    "        \n",
    "        # experience replay\n",
    "        if len(replay) < buffer_size:\n",
    "            replay.append((state, action, reward, new_state))\n",
    "        else:\n",
    "            if h < buffer_size - 1:\n",
    "                h += 1\n",
    "            else:\n",
    "                h = 0 # circular buffer\n",
    "            replay[h] = (state, action, reward, new_state)\n",
    "            \n",
    "            # randomly sample our experience replay memory\n",
    "            minibatch = random.sample(replay, batch_size)\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            \n",
    "            for memory in minibatch:\n",
    "                old_state, action, reward, new_state = memory\n",
    "                old_qval = model.predict(old_state.reshape(1, 64), batch_size=1)\n",
    "                new_Q = model.predict(new_state.reshape(1, 64), batch_size=1)\n",
    "                max_Q = np.max(new_Q)\n",
    "                y = np.zeros((1, 4))\n",
    "                \n",
    "                if reward == -1:\n",
    "                    update = reward + gamma * max_Q\n",
    "                else:\n",
    "                    update = reward\n",
    "                y[0][action] = update\n",
    "                X_train.append(old_state.reshape(64))\n",
    "                y_train.append(y.reshape(4))\n",
    "                \n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "            print('Game #: %s' % i)\n",
    "            \n",
    "            model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1, verbose=1)\n",
    "            state = new_state\n",
    "        if reward != -1:\n",
    "            status = 0\n",
    "        clear_output(wait=True)\n",
    "    if epsilon > 0.1:\n",
    "        epsilon -= (1. / epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid grid. Rebuilding...\n",
      "Initial State:\n",
      "[[u' ' u' ' u'P' u' ']\n",
      " [u'W' u' ' u' ' u' ']\n",
      " [u' ' u' ' u' ' u' ']\n",
      " [u'+' u'-' u' ' u' ']]\n",
      "Move #: 0; Taking action: 1\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u'W' u' ' u'P' u' ']\n",
      " [u' ' u' ' u' ' u' ']\n",
      " [u'+' u'-' u' ' u' ']]\n",
      "Move #: 1; Taking action: 1\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u'W' u' ' u' ' u' ']\n",
      " [u' ' u' ' u'P' u' ']\n",
      " [u'+' u'-' u' ' u' ']]\n",
      "Move #: 2; Taking action: 2\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u'W' u' ' u' ' u' ']\n",
      " [u' ' u'P' u' ' u' ']\n",
      " [u'+' u'-' u' ' u' ']]\n",
      "Move #: 3; Taking action: 2\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u'W' u' ' u' ' u' ']\n",
      " [u'P' u' ' u' ' u' ']\n",
      " [u'+' u'-' u' ' u' ']]\n",
      "Move #: 4; Taking action: 1\n",
      "[[u' ' u' ' u' ' u' ']\n",
      " [u'W' u' ' u' ' u' ']\n",
      " [u' ' u' ' u' ' u' ']\n",
      " [u' ' u'-' u' ' u' ']]\n",
      "Reward: 10\n"
     ]
    }
   ],
   "source": [
    "test_algo(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid grid. Rebuilding...\n",
      "Invalid grid. Rebuilding...\n",
      "Invalid grid. Rebuilding...\n",
      "Invalid grid. Rebuilding...\n",
      "Initial State:\n",
      "[[u' ' u' ' u'P' u'W']\n",
      " [u' ' u' ' u' ' u' ']\n",
      " [u'-' u' ' u'+' u' ']\n",
      " [u' ' u' ' u' ' u' ']]\n",
      "Move #: 0; Taking action: 1\n",
      "[[u' ' u' ' u' ' u'W']\n",
      " [u' ' u' ' u'P' u' ']\n",
      " [u'-' u' ' u'+' u' ']\n",
      " [u' ' u' ' u' ' u' ']]\n",
      "Move #: 1; Taking action: 1\n",
      "[[u' ' u' ' u' ' u'W']\n",
      " [u' ' u' ' u' ' u' ']\n",
      " [u'-' u' ' u' ' u' ']\n",
      " [u' ' u' ' u' ' u' ']]\n",
      "Reward: 10\n"
     ]
    }
   ],
   "source": [
    "test_algo(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
